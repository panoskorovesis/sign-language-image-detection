{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "print_every = 100\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 55500\n",
      "Training set size: 38850\n",
      "Validation set size: 11100\n",
      "Test set size: 5550\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the dataset\n",
    "dataset_path = \"../../sign_datasets/sign-language-gesture-images-dataset/Gesture Image Data\"\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Load the dataset without transformations\n",
    "dataset = datasets.ImageFolder(root=dataset_path)\n",
    "\n",
    "# Get the total number of samples\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.7 * total_samples)\n",
    "val_size = int(0.2 * total_samples)\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "# Print the sizes of each split\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Training set size: {train_size}\")\n",
    "print(f\"Validation set size: {val_size}\")\n",
    "print(f\"Test set size: {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_']\n",
      "Train Batch - Images shape: torch.Size([64, 3, 50, 50])\n",
      "Train Batch - Labels shape: torch.Size([64])\n",
      "Train set size: 38850\n",
      "Validation set size: 11100\n",
      "Test set size: 5550\n",
      "Train DataLoader size: 608\n",
      "Validation DataLoader size: 174\n",
      "Test DataLoader size: 87\n"
     ]
    }
   ],
   "source": [
    "# Already computed mean and std\n",
    "computed_mean = [0.5273298025131226, 0.4507707953453064, 0.4120909869670868]\n",
    "computed_std = [0.5632718205451965, 0.5085217952728271, 0.48751917481422424]\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((X, Y)),  # Resize images to X,Y if necessary\n",
    "    transforms.ToTensor(),       # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=computed_mean, std=computed_std)  # Normalize\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Get class names\n",
    "class_names = dataset.classes\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "# Example of iterating through the train DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(\"Train Batch - Images shape:\", images.shape)\n",
    "    print(\"Train Batch - Labels shape:\", labels.shape)\n",
    "    break\n",
    "\n",
    "# Print dataset sizes for verification\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "len_train_loader = len(train_loader)\n",
    "len_val_loader = len(val_loader)\n",
    "len_test_loader = len(test_loader)\n",
    "\n",
    "# Print DataLoader sizes for verification\n",
    "print(f\"Train DataLoader size: {len(train_loader)}\")\n",
    "print(f\"Validation DataLoader size: {len(val_loader)}\")\n",
    "print(f\"Test DataLoader size: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part(loader, model, return_acc = False):\n",
    "    # Determine the split type\n",
    "    if len(loader) == len_train_loader:\n",
    "        split = 'train'\n",
    "    elif len(loader) == len_val_loader:\n",
    "        split = 'val'\n",
    "    elif len(loader) == len_test_loader:\n",
    "        split = 'test'\n",
    "\n",
    "    print(f'Checking accuracy on the {split} set')\n",
    "\n",
    "    # init counters\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()  \n",
    "\n",
    "     # Disable gradient computation as we dont train\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype) # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)  # use torch.long for labels\n",
    "\n",
    "            # forward pass\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)  # get predicted class -> highest score\n",
    "\n",
    "            # Count correct predictions and total samples\n",
    "            correct_predictions = preds == y  # True if correct - False otherwise\n",
    "            num_correct_batch = correct_predictions.sum().item() # count the correct predictions\n",
    "            num_correct = num_correct + num_correct_batch\n",
    "\n",
    "            num_samples_batch = preds.size(0)  # get the batch size\n",
    "            num_samples = num_samples + num_samples_batch\n",
    "\n",
    "    # compute and print accuracy\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))\n",
    "    \n",
    "    if return_acc == True:\n",
    "        return 100 * acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part(model, optimizer, epochs=1, return_acc = False, check_train_set = False, check_test_set = False):\n",
    "\n",
    "    # List to store validation and train accuracies at each epoch where needed\n",
    "    val_accs = []\n",
    "    train_accs = []\n",
    "\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part(val_loader, model)\n",
    "                print()\n",
    "\n",
    "        # After each epoch, check accuracies\n",
    "        val_acc = check_accuracy_part(val_loader, model, return_acc=True)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"Validation Accuracy after epoch {e+1}: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Return accuracies if requested\n",
    "        if check_train_set:\n",
    "            train_acc = check_accuracy_part(train_loader, model, return_acc=True)\n",
    "            train_accs.append(train_acc)\n",
    "            print(f\"Training Accuracy after epoch {e+1}: {train_acc:.2f}%\")\n",
    "\n",
    "    # Return accuracies if requested\n",
    "    if return_acc:\n",
    "        if check_train_set:  # If requested, return both validation and training accuracies\n",
    "            return val_accs, train_accs\n",
    "        if check_test_set:  # If requested, return the validation accuracy and test accuracy\n",
    "            test_acc = check_accuracy_part(test_loader, model, return_acc=True)  # Check test set accuracy\n",
    "            return val_accs[-1], test_acc  # Return the most recent validation accuracy and the test accuracy\n",
    "        return val_accs  # Return only validation accuracies if no other set (train or test) is specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.6132\n",
      "Checking accuracy on the val set\n",
      "Got 329 / 11100 correct (2.96%)\n",
      "\n",
      "Iteration 100, loss = 0.3234\n",
      "Checking accuracy on the val set\n",
      "Got 9825 / 11100 correct (88.51%)\n",
      "\n",
      "Iteration 200, loss = 0.1308\n",
      "Checking accuracy on the val set\n",
      "Got 10614 / 11100 correct (95.62%)\n",
      "\n",
      "Iteration 300, loss = 0.0346\n",
      "Checking accuracy on the val set\n",
      "Got 10964 / 11100 correct (98.77%)\n",
      "\n",
      "Iteration 400, loss = 0.0667\n",
      "Checking accuracy on the val set\n",
      "Got 11001 / 11100 correct (99.11%)\n",
      "\n",
      "Iteration 500, loss = 0.0054\n",
      "Checking accuracy on the val set\n",
      "Got 11007 / 11100 correct (99.16%)\n",
      "\n",
      "Iteration 600, loss = 0.0124\n",
      "Checking accuracy on the val set\n",
      "Got 11065 / 11100 correct (99.68%)\n",
      "\n",
      "Checking accuracy on the val set\n",
      "Got 11021 / 11100 correct (99.29%)\n",
      "Validation Accuracy after epoch 1: 99.29%\n",
      "Checking accuracy on the test set\n",
      "Got 5506 / 5550 correct (99.21%)\n",
      "\n",
      "########### Final Results ###########\n",
      "Validation Accuracy: 99.28828828828829\n",
      "Test Accuracy: 99.2072072072072\n"
     ]
    }
   ],
   "source": [
    "# 1. Convolutional layer (with bias) with 32 5x5 filters, with zero-padding of 2\n",
    "# 2. ReLU\n",
    "# 3. Convolutional layer (with bias) with 16 3x3 filters, with zero-padding of 1\n",
    "# 4. ReLU\n",
    "# 5. Fully-connected layer (with bias) to compute scores for the classes\n",
    "\n",
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "in_channel = 3\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channel, channel_1, kernel_size=5, padding=2), # First conv layer with Relu\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1), # Second conv layer with Relu\n",
    "    nn.ReLU(),                                             \n",
    "    nn.Flatten(), # Flatten the output\n",
    "    nn.Linear(channel_2 * 50 * 50, num_classes) # Fully-connected layer, (channel_2 * 50 * 50) -> the number of features coming into the fully connected layer.\n",
    ")\n",
    "\n",
    "# You should optimize your model using stochastic gradient descent with Nesterov momentum 0.9.\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "val_acc, test_acc = train_part(model, optimizer, return_acc=True, check_test_set = True)\n",
    "\n",
    "\n",
    "# Print the validation and test accuracies\n",
    "print()\n",
    "print('########### Final Results ###########')\n",
    "print(\"Validation Accuracy:\", val_acc)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state_dict\n",
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_409000/2237739796.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('model.pth')  # Load the entire model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): Flatten(start_dim=1, end_dim=-1)\n",
       "  (5): Linear(in_features=40000, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('model.pth')  # Load the entire model\n",
    "\n",
    "# Move the model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to the image\n",
    "#image_path = \"../../sign_datasets/sign-language-gesture-images-dataset/Gesture Image Data/_/67.jpg\"\n",
    "#image_path = \"../../sign_datasets/sign-language-gesture-images-dataset/Gesture Image Pre-Processed Data/A/67.jpg\"\n",
    "image_path = \"../../sign_datasets/custom-dataset/0/0.jpg\"\n",
    "\n",
    "# Define the transformations (same as during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=computed_mean, std=computed_std)\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(image_path)\n",
    "image = image.convert(\"RGB\")  # Ensure 3-channel RGB\n",
    "image = image.resize((50, 50))  # Resize to match the input size if necessary\n",
    "\n",
    "# Plot the resized image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "# Apply the transformations\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: J\n"
     ]
    }
   ],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)  # Raw scores (logits)\n",
    "    predicted_idx = torch.argmax(output, dim=1).item()  # Get the index of the highest logit\n",
    "    predicted_class = class_names[predicted_idx]  # Map to class name\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
