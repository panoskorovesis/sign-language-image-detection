{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision.models.inception import InceptionOutputs\n",
    "import time\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "print_every = -1\n",
    "print('using device:', device)\n",
    "\n",
    "dropout_flag = True\n",
    "dropout_prob = 0.5\n",
    "if dropout_flag:\n",
    "    path_to_store_the_plots = 'plots/conv2d_with_dropout/'\n",
    "    \n",
    "else:\n",
    "    path_to_store_the_plots = 'plots/conv2d/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define a transform without normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Convert NumPy array to PyTorch tensor\n",
    "])\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        self.X = X  # Keep as NumPy array\n",
    "        self.transform = transform  # Store the transform\n",
    "        \n",
    "        # Convert string labels into numeric using LabelEncoder\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.Y = [str(y[0]) for y in Y]  # Ensure Y is a list of strings\n",
    "        self.classes = list(sorted(set(self.Y)))  # Unique class names\n",
    "        self.Y = self.label_encoder.fit_transform(self.Y)  # Convert to numeric labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]  # NumPy array\n",
    "        y = self.Y[idx]  # Numeric label (integer)\n",
    "\n",
    "        # Apply the transform to the image if provided\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 64\n",
    "\n",
    "dataset_path = \"../../../sign_datasets/27-class-sign-language-dataset/\"\n",
    "path_to_store_the_plots = path_to_store_the_plots + dataset_path.split('/')[-1]\n",
    "\n",
    "# Check if the directory exists, and create it if it doesn't\n",
    "os.makedirs(path_to_store_the_plots, exist_ok=True)\n",
    "\n",
    "print(f\"Plots will be stored in: {path_to_store_the_plots}\")\n",
    "\n",
    "# Step 1: Load the .npy files\n",
    "X = np.load(dataset_path + \"X.npy\")  # Shape: (Samples, H, W, C), dtype: float\n",
    "Y = np.load(dataset_path + \"Y.npy\")  # Shape: (Samples, 1), dtype: str\n",
    "\n",
    "# Step 3: Instantiate the dataset\n",
    "# Instantiate the dataset with the transform\n",
    "dataset = NumpyDataset(X, Y, transform=transform)\n",
    "\n",
    "# Step 4: Split the dataset into train, validation, and test sets\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.7 * total_samples)  # 70% for training\n",
    "val_size = int(0.2 * total_samples)    # 20% for validation\n",
    "test_size = total_samples - train_size - val_size  # Remaining for testing\n",
    "\n",
    "# Perform the split\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Step 5: Print the sizes of each split\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Training set size: {train_size}\")\n",
    "print(f\"Validation set size: {val_size}\")\n",
    "print(f\"Test set size: {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already computed mean and std\n",
    "computed_mean = [0.5108343362808228, 0.47160401940345764, 0.43902790546417236]\n",
    "computed_std = [0.5867133736610413, 0.5466195344924927, 0.5143911242485046]\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),       # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=computed_mean, std=computed_std)  # Normalize\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = NumpyDataset(X, Y, transform=transform)\n",
    "\n",
    "# Define class names based on unique labels in Y\n",
    "class_names = list(set([str(y[0]) for y in Y]))\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes for verification\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "len_train_loader = len(train_loader)\n",
    "len_val_loader = len(val_loader)\n",
    "len_test_loader = len(test_loader)\n",
    "\n",
    "# Print DataLoader sizes for verification\n",
    "print(f\"Train DataLoader size: {len(train_loader)}\")\n",
    "print(f\"Validation DataLoader size: {len(val_loader)}\")\n",
    "print(f\"Test DataLoader size: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the first image in the dataset\n",
    "first_image, _ = train_loader.dataset[0]\n",
    "\n",
    "# Extract dimensions\n",
    "width = first_image.shape[1]\n",
    "height = first_image.shape[2]\n",
    "\n",
    "print(f\"Width: {width}, Height: {height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part(loader, model, return_acc=False):\n",
    "\n",
    "    # Determine the split type\n",
    "    if len(loader) == len_train_loader:\n",
    "        split = 'train'\n",
    "    elif len(loader) == len_val_loader:\n",
    "        split = 'val'\n",
    "    elif len(loader) == len_test_loader:\n",
    "        split = 'test'\n",
    "\n",
    "    print(f'Checking accuracy on the {split} set')\n",
    "\n",
    "    # Init counters and placeholders\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation as we don't train\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # Move to device, e.g., GPU\n",
    "            y = y.to(device=device, dtype=torch.long)  # Use torch.long for labels\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)  # Get predicted class -> highest score\n",
    "\n",
    "            # Collect all predictions and labels for confusion matrix\n",
    "            all_preds.append(preds.cpu())  # Append to list, move to CPU for numpy\n",
    "            all_labels.append(y.cpu())    # Append to list, move to CPU for numpy\n",
    "\n",
    "            # Count correct predictions and total samples\n",
    "            correct_predictions = preds == y  # True if correct - False otherwise\n",
    "            num_correct += correct_predictions.sum().item()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "    # Compute accuracy\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "    # For test set: plot confusion matrix\n",
    "    if split == 'test':\n",
    "        all_preds = torch.cat(all_preds).numpy()  # Flatten and convert to numpy\n",
    "        all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        np.savetxt(path_to_store_the_plots + '/confusion_matrix.txt', conf_matrix, fmt='%d')\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        sns.heatmap(\n",
    "            conf_matrix,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Greens',\n",
    "            linewidths=0.5,\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names\n",
    "        )\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix with Actual Class Labels')\n",
    "        plt.savefig(path_to_store_the_plots + '/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    if return_acc:\n",
    "        return 100 * acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part(model, optimizer, epochs=1, scheduler = None, return_acc = False, check_train_set = False, check_test_set = False, early_stopping = False, patience = 5):\n",
    "    \n",
    "    # List to store validation and train accuracies at each epoch where needed\n",
    "    val_accs = []\n",
    "    train_accs = []\n",
    "    losses = []\n",
    "    val_losses = [] \n",
    "    best_val_loss = float('inf')  # Initialize with a high value\n",
    "    \n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t+1 % print_every == 0 and print_every!=-1 :\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part(val_loader, model)\n",
    "                print()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        print(f\"Training Loss after epoch {e+1}: {loss.item()}\")\n",
    "\n",
    "        # Step the scheduler after each epoch to adjust the learning rate if needed\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                # If the scheduler is of type ReduceLROnPlateau, use validation loss for adjustment\n",
    "                model.eval()  # Set model to evaluation mode\n",
    "                val_loss = 0.0  # Initialize validation loss accumulator\n",
    "                with torch.no_grad():  # No need to compute gradients during validation\n",
    "                    for data, target in val_loader:\n",
    "                        data, target = data.to(device=device, dtype=dtype), target.to(device=device, dtype=torch.long)\n",
    "                        output = model(data)\n",
    "                        loss = F.cross_entropy(output, target)\n",
    "                        val_loss += loss.item()  # Accumulate the validation loss\n",
    "\n",
    "                val_loss /= len(val_loader)  # Average the validation loss\n",
    "                scheduler.step(val_loss)  # Step the scheduler based on the validation loss\n",
    "            else:\n",
    "                scheduler.step()  # Step the scheduler for other types of schedulers\n",
    "\n",
    "        # Calculate validation loss for the epoch\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device=device, dtype=dtype), target.to(device=device, dtype=torch.long)\n",
    "                output = model(data)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)  # Average validation loss\n",
    "        val_losses.append(val_loss)  # Append validation loss to list\n",
    "\n",
    "        # After each epoch, check accuracies\n",
    "        val_acc = check_accuracy_part(val_loader, model, return_acc=True)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"Validation Accuracy after epoch {e+1}: {val_acc:.2f}%\")\n",
    "        print(f\"Validation Loss after epoch {e+1}: {val_loss:.2f}\")\n",
    "        \n",
    "        # Return accuracies if requested\n",
    "        if check_train_set:\n",
    "            train_acc = check_accuracy_part(train_loader, model, return_acc=True)\n",
    "            train_accs.append(train_acc)\n",
    "            print(f\"Training Accuracy after epoch {e+1}: {train_acc:.2f}%\")\n",
    "\n",
    "        if early_stopping:\n",
    "            # Early stopping logic\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Early stopping at epoch {e+1}!\")\n",
    "                    break\n",
    "\n",
    "    # Return accuracies if requested\n",
    "    if return_acc:\n",
    "        if check_train_set and check_test_set:\n",
    "            test_acc = check_accuracy_part(test_loader, model, return_acc=True)  # Check test set accuracy\n",
    "            return val_accs, train_accs, test_acc, losses, val_losses  # Return the most recent validation accuracy and the test accuracy\n",
    "        if check_train_set:  # If requested, return both validation and training accuracies\n",
    "            return val_accs, train_accs, losses, val_losses\n",
    "        if check_test_set:  # If requested, return the validation accuracy and test accuracy\n",
    "            test_acc = check_accuracy_part(test_loader, model, return_acc=True)  # Check test set accuracy\n",
    "            return val_accs, test_acc, losses, val_losses  # Return the most recent validation accuracy and the test accuracy\n",
    "        return val_accs, losses, val_losses  # Return only validation accuracies if no other set (train or test) is specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "in_channel = 3\n",
    "num_classes = len(class_names)\n",
    "\n",
    "if dropout_flag:\n",
    "    print('Train With Dropout.')\n",
    "\n",
    "    # Define the model using nn.Sequential\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_channel, channel_1, kernel_size=5, padding=2),\n",
    "        nn.BatchNorm2d(channel_1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # Add pooling to reduce spatial dimensions\n",
    "        nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(channel_2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # Add pooling to reduce spatial dimensions\n",
    "        nn.Flatten(),\n",
    "        nn.Dropout(dropout_prob),\n",
    "        nn.Linear(channel_2 * (width // 4) * (height // 4), num_classes)  # Adjust input size due to pooling\n",
    "    )\n",
    "    early_stopping = True\n",
    "\n",
    "else:\n",
    "    print('Train Without Dropout.')\n",
    "    # Define the model using nn.Sequential\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_channel, channel_1, kernel_size=5, padding=2), # First conv layer with Relu\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1), # Second conv layer with Relu\n",
    "        nn.ReLU(),                                             \n",
    "        nn.Flatten(), # Flatten the output\n",
    "        nn.Linear(channel_2 * width * height, num_classes) # Fully-connected layer, (channel_2 * 50 * 50) -> the number of features coming into the fully connected layer.\n",
    "    )\n",
    "    early_stopping = False\n",
    "    \n",
    "# You should optimize your model using stochastic gradient descent with Nesterov momentum 0.9.\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "val_accs, train_accs, test_acc, losses, val_losses = train_part(model, optimizer,epochs=20, return_acc=True, check_train_set = True, check_test_set = True, early_stopping=early_stopping)\n",
    "# Stop the timer\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print the validation and test accuracies\n",
    "print()\n",
    "print('########### Final Results ###########')\n",
    "print(\"Elapsed Time: {:.2f} seconds\".format(elapsed_time))\n",
    "print(\"Validation Accuracy:\", val_accs[-1])\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "epochs = range(len(val_accs))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_accs, 'r', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accs, 'b', label='Validation Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig(path_to_store_the_plots + '/Accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, losses, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.savefig(path_to_store_the_plots + '/Loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize placeholders for features and labels\n",
    "features, labels_list = [], []\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        # Move data to the appropriate device\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Forward pass to extract features\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Handle Inception model outputs\n",
    "        if isinstance(outputs, InceptionOutputs):\n",
    "            outputs = outputs.logits\n",
    "        \n",
    "        # Collect features and labels\n",
    "        features.append(outputs.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "        labels_list.extend(labels.numpy())  # Add labels to list\n",
    "\n",
    "# Concatenate all features\n",
    "features = np.concatenate(features, axis=0)\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "features_2d = tsne.fit_transform(features)\n",
    "\n",
    "# Visualize with a scatter plot\n",
    "plt.figure(figsize=(16, 16))\n",
    "scatter = sns.scatterplot(\n",
    "    x=features_2d[:, 0],\n",
    "    y=features_2d[:, 1],\n",
    "    hue=labels_list,\n",
    "    palette=sns.color_palette(\"hsv\", len(dataset.classes)),\n",
    "    legend='full',\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('t-SNE Feature 1')\n",
    "plt.ylabel('t-SNE Feature 2')\n",
    "plt.title('t-SNE Visualization of Feature Representations')\n",
    "\n",
    "# Add a color bar for better visualization\n",
    "scatter.figure.colorbar(scatter.collections[0], label='Class Labels')\n",
    "plt.savefig(path_to_store_the_plots + '/t-SNE.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
